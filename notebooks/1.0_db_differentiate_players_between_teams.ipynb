{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbdb2665a77916f",
   "metadata": {},
   "source": [
    "# Differentiate players between teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f35b9f82c969d5",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef7f680cf2860a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:46.293985Z",
     "start_time": "2025-01-15T01:58:46.041984Z"
    }
   },
   "outputs": [],
   "source": [
    "from ai.config import PROJ_ROOT\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7a7248306a79",
   "metadata": {},
   "source": [
    "## Install dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74693e3d103a1d",
   "metadata": {},
   "source": [
    "Let's install PyTorch library, but make sure you're installing version compatible with your environment: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefce0b021e231ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:48.314108Z",
     "start_time": "2025-01-15T01:58:46.586290Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33ed5fcf5e1c01",
   "metadata": {},
   "source": [
    "Let's install ultralytics library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2189b86b1bb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:50.112942Z",
     "start_time": "2025-01-15T01:58:48.321091Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406b26de03fdd0",
   "metadata": {},
   "source": [
    "Let's make sure we have the latest features in the supervision library by installing version `0.23.0` or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b219165fdbf94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:51.817188Z",
     "start_time": "2025-01-15T01:58:50.124322Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install supervision==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac316b268009ef2",
   "metadata": {},
   "source": [
    "Let's install transformers and sentencepiece libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962af081e9f97d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:54.893948Z",
     "start_time": "2025-01-15T01:58:51.829131Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install 'transformers[torch]'\n",
    "!python -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f09494b7c468d",
   "metadata": {},
   "source": [
    "Let's install numpy and more-itertools libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f99f76e31d8bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:58:56.831400Z",
     "start_time": "2025-01-15T01:58:54.902136Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install numpy\n",
    "!python -m pip install more-itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbc0ce9f1edfb5",
   "metadata": {},
   "source": [
    "Let's install scikit-learn and umap-learn libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be844cccbdf8e0d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:00.058687Z",
     "start_time": "2025-01-15T01:58:56.838227Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install -U scikit-learn\n",
    "!python -m pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97d3363765533e",
   "metadata": {},
   "source": [
    "## Load player detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4e6dd4c029854a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:03.961734Z",
     "start_time": "2025-01-15T01:59:00.068550Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "NOTEBOOK_PATH = Path(os.getcwd())\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "\n",
    "PLAYER_DETECTION_MODEL_PATH = \"../models/player_inference.pt\"\n",
    "PLAYER_DETECTION_MODEL = YOLO(PLAYER_DETECTION_MODEL_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c0da370ef155f",
   "metadata": {},
   "source": [
    "## Split players into teams\n",
    "\n",
    "### Gathering training data\n",
    "\n",
    "To gather training data, we'll sample one frame per second, detect players within those frames, and then crop them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c67cdf5a3c84e7fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:04.200017Z",
     "start_time": "2025-01-15T01:59:03.973353Z"
    }
   },
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PLAYER_ID = 1\n",
    "STRIDE = 30\n",
    "\n",
    "def extract_crops(source_video_path: str) -> list:\n",
    "    frame_generator = sv.get_video_frames_generator(source_path=source_video_path, stride=STRIDE)\n",
    "\n",
    "    crops = []\n",
    "\n",
    "    for frame in tqdm(frame_generator, desc=\"collecting crops\"):\n",
    "        result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
    "\n",
    "        detections = sv.Detections.from_ultralytics(result)\n",
    "        detections = detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "        detections = detections[detections.class_id == PLAYER_ID]\n",
    "\n",
    "        players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "        crops += players_crops\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5275238756010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:14.923833Z",
     "start_time": "2025-01-15T01:59:04.210351Z"
    }
   },
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = \"../data/test.mp4\"\n",
    "crops = extract_crops(SOURCE_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fbfcbd89d70ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:14.972555Z",
     "start_time": "2025-01-15T01:59:14.968093Z"
    }
   },
   "outputs": [],
   "source": [
    "len(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38fe6c64d46768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:15.948807Z",
     "start_time": "2025-01-15T01:59:15.017933Z"
    }
   },
   "outputs": [],
   "source": [
    "sv.plot_images_grid(crops[:100], grid_size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78aae66888cc04",
   "metadata": {},
   "source": [
    "### Calculating embeddings for each of the crops using SigLip\n",
    "\n",
    "Let's load SigLip model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950db4ad9140956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:17.737192Z",
     "start_time": "2025-01-15T01:59:16.020569Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, SiglipVisionModel\n",
    "\n",
    "\n",
    "def load_siglip_model():\n",
    "    SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'\n",
    "\n",
    "    EMBEDDINGS_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
    "    EMBEDDINGS_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)\n",
    "\n",
    "    return EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655628be8303779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:20.973090Z",
     "start_time": "2025-01-15T01:59:17.746292Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR = load_siglip_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd658fd9e521c113",
   "metadata": {},
   "source": [
    "Let's run SigLip model on crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0eeccfc0a7c511a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:21.011828Z",
     "start_time": "2025-01-15T01:59:21.008823Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def extract_features(crops: list, embedding_model, embedding_processor) -> np.ndarray:\n",
    "    crops = [sv.cv2_to_pillow(crop) for crop in crops]\n",
    "    batches = chunked(crops, BATCH_SIZE)\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches, desc='embedding extraction'):\n",
    "            inputs = embedding_processor(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
    "            outputs = embedding_model(**inputs)\n",
    "\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "            data.append(embeddings)\n",
    "\n",
    "    data = np.concatenate(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bee4da2249b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:25.999825Z",
     "start_time": "2025-01-15T01:59:21.051891Z"
    }
   },
   "outputs": [],
   "source": [
    "data = extract_features(crops, EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2f44cf7d74c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:26.040727Z",
     "start_time": "2025-01-15T01:59:26.037840Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec22922f9e4c6c",
   "metadata": {},
   "source": [
    "### Projecting our embeddings from (N, 768) to (N, 3) using UMAP and performing a two-cluster division using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f164012778c5ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:38.578705Z",
     "start_time": "2025-01-15T01:59:26.082038Z"
    }
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "REDUCER = umap.UMAP(n_components=3)\n",
    "CLUSTERING_MODEL = KMeans(n_clusters=2)\n",
    "\n",
    "projections = REDUCER.fit_transform(data)\n",
    "clusters = CLUSTERING_MODEL.fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66597fc1d33dcaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:38.624407Z",
     "start_time": "2025-01-15T01:59:38.621447Z"
    }
   },
   "outputs": [],
   "source": [
    "projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa1c0c01b53c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:38.691376Z",
     "start_time": "2025-01-15T01:59:38.687750Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "841849c0fcb0c92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:38.740279Z",
     "start_time": "2025-01-15T01:59:38.737228Z"
    }
   },
   "outputs": [],
   "source": [
    "team_0 = [\n",
    "    crop\n",
    "    for crop, cluster\n",
    "    in zip(crops, clusters)\n",
    "    if cluster == 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc8a748e3d180f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:39.520439Z",
     "start_time": "2025-01-15T01:59:38.782001Z"
    }
   },
   "outputs": [],
   "source": [
    "sv.plot_images_grid(team_0[:100], grid_size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d1ce4bccdcdc1",
   "metadata": {},
   "source": [
    "### Assigning goalkeepers to teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d52935271bf81e79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:39.570356Z",
     "start_time": "2025-01-15T01:59:39.567084Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "def resolve_goalkeepers_team_id(players: sv.Detections, goalkeepers: sv.Detections) -> np.ndarray:\n",
    "    goalkeepers_xy = goalkeepers.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "    players_xy = players.get_anchors_coordinates(sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    team_0_centroid = players_xy[players.class_id == 0].mean(axis=0)\n",
    "    team_1_centroid = players_xy[players.class_id == 1].mean(axis=0)\n",
    "\n",
    "    goalkeepers_team_id = []\n",
    "\n",
    "    for goalkeeper_xy in goalkeepers_xy:\n",
    "        dist_0 = np.linalg.norm(goalkeeper_xy - team_0_centroid)\n",
    "        dist_1 = np.linalg.norm(goalkeeper_xy - team_1_centroid)\n",
    "\n",
    "        goalkeepers_team_id.append(0 if dist_0 < dist_1 else 1)\n",
    "\n",
    "    return np.array(goalkeepers_team_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d4c12e46d26e9",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ee90982678b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:50.837821Z",
     "start_time": "2025-01-15T01:59:39.617300Z"
    }
   },
   "outputs": [],
   "source": [
    "crops = extract_crops(SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Fit\n",
    "EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR = load_siglip_model()\n",
    "data = extract_features(crops, EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR)\n",
    "\n",
    "REDUCER = umap.UMAP(n_components=3)\n",
    "CLUSTERING_MODEL = KMeans(n_clusters=2)\n",
    "\n",
    "projections = REDUCER.fit_transform(data)\n",
    "CLUSTERING_MODEL.fit(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e52126e44f75c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T01:59:56.786779Z",
     "start_time": "2025-01-15T01:59:50.892978Z"
    }
   },
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"../data/test.mp4\"\n",
    "GOALKEEPER_ID = 0\n",
    "PLAYER_ID = 1\n",
    "REFEREE_ID = 2\n",
    "\n",
    "ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.predict(frame, conf=0.3)[0]\n",
    "detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "all_detections = detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "all_detections = tracker.update_with_detections(detections=all_detections)\n",
    "\n",
    "goalkeepers_detections = all_detections[all_detections.class_id == GOALKEEPER_ID]\n",
    "players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
    "referees_detections = all_detections[all_detections.class_id == REFEREE_ID]\n",
    "\n",
    "players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
    "\n",
    "# Predict\n",
    "data = extract_features(players_crops, EMBEDDINGS_MODEL, EMBEDDINGS_PROCESSOR)\n",
    "projections = REDUCER.transform(data)\n",
    "players_detections.class_id = CLUSTERING_MODEL.predict(projections)\n",
    "\n",
    "goalkeepers_detections.class_id = resolve_goalkeepers_team_id(players_detections, goalkeepers_detections)\n",
    "\n",
    "all_detections = sv.Detections.merge([players_detections, goalkeepers_detections, referees_detections])\n",
    "\n",
    "labels = [\n",
    "    f\"#{tracker_id}\"\n",
    "    for tracker_id\n",
    "    in all_detections.tracker_id\n",
    "]\n",
    "\n",
    "all_detections.class_id = all_detections.class_id.astype(int)\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "\n",
    "annotated_frame = ellipse_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=all_detections)\n",
    "\n",
    "annotated_frame = label_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=all_detections,\n",
    "    labels=labels)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
